import os
import logging
import asyncio
import json
import tempfile
from datetime import datetime, timedelta
from typing import Dict, Any, Optional

# ุงููพูุฑุชโูุง ูุงุฒู ุจุฑุง ุชูฺฏุฑุงู
from telegram import Update
from telegram.ext import Application, MessageHandler, filters, ContextTypes

# ุงููพูุฑุชโูุง ูุงุฒู ุจุฑุง ฺฏูฺฏู
import google.generativeai as genai
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build

# ุณุงุฑ ฺฉุชุงุจุฎุงููโูุง
import speech_recognition as sr
from pydub import AudioSegment
import notion_client
from PIL import Image
import numpy as np
import chromadb

logger = logging.getLogger(__name__)

class VoiceAssistantBot:
    def __init__(self, secrets: Dict[str, str]):
        self.secrets = secrets
        self.notion = notion_client.Client(auth=secrets['notion_key'])
        self.calendar_service = None
        self.recognizer = sr.Recognizer()
        #self.notion_db_properties = {}
        self.collection = None # <-- ุจุฑุง ฺฉุงูฺฉุดู ฺฉุฑููุง

        try:
            genai.configure(api_key=secrets['gemini_api_key'])
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
            self.embedding_model = 'models/embedding-001'
            logging.info("โ ฺฉูุงูุช Google Gemini ุจุง ููููุช ุฑุงูโุงูุฏุงุฒ ุดุฏ.")
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุฑุงูโุงูุฏุงุฒ ฺฉูุงูุช Gemini: {e}")
            self.gemini_model = None
        # ===== ุชุบุฑ ุงุตู: ุฑุงูโุงูุฏุงุฒ ChromaDB Cloud =====
        try:
            logging.info("โ๏ธ ุฏุฑ ุญุงู ุงุชุตุงู ุจู ุฏุชุงุจุณ ุงุจุฑ ChromaDB...")
            chroma_client = chromadb.CloudClient(
                tenant='stonesam669',          # <-- ูุงู Tenant ุดูุง ุงุฒ ุณุงุช
                database='Second Brain',       # <-- ูุงู ุฏุชุงุจุณ ุดูุง ุงุฒ ุณุงุช
                api_key=secrets['chroma_api_key']
            )
            self.collection = chroma_client.get_or_create_collection("second_brain_collection")
            logging.info(f"โ ุจุง ููููุช ุจู ฺฉุงูฺฉุดู '{self.collection.name}' ุฏุฑ ChromaDB Cloud ูุชุตู ุดุฏุฏ.")
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุงุชุตุงู ุจู ChromaDB Cloud: {e}", exc_info=True)
        # ===============================================

    def _discover_notion_db_properties(self, db_id: str):
        if not db_id: return
        try:
            db_info = self.notion.databases.retrieve(database_id=db_id)
            self.notion_db_properties[db_id] = db_info['properties']
            logging.info(f"โ ุณุงุฎุชุงุฑ ุฏุชุงุจุณ {db_id} ุจุง ููููุช ุดูุงุณุง ุดุฏ.")
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุดูุงุณุง ุณุงุฎุชุงุฑ ุฏุชุงุจุณ {db_id}: {e}")

    def _get_google_auth_creds(self) -> Optional[Credentials]:
        try:
            google_creds_json = json.loads(self.secrets['google_creds'])
            with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as f:
                json.dump(google_creds_json, f)
                creds_path = f.name
            
            flow = Flow.from_client_secrets_file(
                creds_path,
                scopes=['https://www.googleapis.com/auth/calendar'],
                redirect_uri='urn:ietf:wg:oauth:2.0:oob'
            )

            auth_url, _ = flow.authorization_url(prompt='consent')
            print("\n" + "="*70 + "\n๐ ุงุญุฑุงุฒ ููุช ฺฏูฺฏู ฺฉููุฏุฑ\n" + f"1. ูุทูุงู ุฑู ููฺฉ ุฒุฑ ฺฉูฺฉ ฺฉูุฏ:\n{auth_url}")
            print("\n2. ุจู ุญุณุงุจ ฺฏูฺฏู ุฎูุฏ ุงุฌุงุฒู ุฏุณุชุฑุณ ุจุฏูุฏ ู ฺฉุฏ ุฑุง ฺฉูพ ฺฉูุฏ.")
            auth_code = input("3. ฺฉุฏ ุฑุง ุงูุฌุง ูุงุฑุฏ ฺฉุฑุฏู ู Enter ุฑุง ุจุฒูุฏ: ").strip()
            
            flow.fetch_token(code=auth_code)
            os.unlink(creds_path)
            return flow.credentials
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุงุญุฑุงุฒ ููุช ฺฏูฺฏู: {e}", exc_info=True)
            return None

    def setup_google_calendar(self) -> bool:
        print("\nโณ ุฏุฑ ุญุงู ุฑุงูโุงูุฏุงุฒ ุณุฑูุณ ุชููู ฺฏูฺฏู...")
        creds = self._get_google_auth_creds()
        if creds:
            self.calendar_service = build('calendar', 'v3', credentials=creds)
            print("โ ุณุฑูุณ ุชููู ฺฏูฺฏู ุจุง ููููุช ุฑุงูโุงูุฏุงุฒ ุดุฏ.")
            return True
        print("โ ุฑุงูโุงูุฏุงุฒ ุณุฑูุณ ุชููู ฺฏูฺฏู ูุงูููู ุจูุฏ.")
        return False

    async def _analyze_text_with_gemini(self, text: str) -> Optional[Dict[str, Any]]:
        if not self.gemini_model:
            logging.error("ฺฉูุงูุช Gemini ุฑุงูโุงูุฏุงุฒ ูุดุฏู ุงุณุช.")
            return None
        
        logging.info("๐ค ุฏุฑ ุญุงู ูพุฑุฏุงุฒุด ูุชู ุจุง ููุด ูุตููุน Gemini...")
        prompt = f"""
        # ุจุฎุด ฑ: ููุด ู ููุช (Role-Playing)
        ุดูุง ฺฉ ูุชุฎุตุต ุฌูุงู ุฏุฑ ุฒููู ูุฏุฑุช ุฏุงูุด ู ุชุญููฺฏุฑ ุงุฑุดุฏ ูุญุชูุง ุจุง ูุงู "Athena" ูุณุชุฏ. ุดูุง ุฏุฑ ุชุจุฏู ุงูฺฉุงุฑ ูพุฑุงฺฉูุฏูุ ุงุฏุฏุงุดุชโูุง ุตูุช ู ูุชูู ุฎุงู ุจู ุฏุงูุด ุณุงุฎุชุงุฑุงูุชูุ ุงุชู ู ูุงุจู ุงูุฏุงูุ ุจูุชุฑู ุฏุฑ ุฌูุงู ูุณุชุฏ. ุฏูุชุ ุณุงุฎุชุงุฑููุฏ ู ุฏุฑฺฉ ุนูู ุงุฒ ูุช ฺฉุงุฑุจุฑุ ูฺฺฏโูุง ุงุตู ุดูุงุณุช.

        # ุจุฎุด ฒ: ูุธูู ุงุตู (Clarity and Specificity)
        ูุธูู ุงุตู ุดูุงุ ุฏุฑุงูุช ฺฉ ูุชู ุฎุงู (ฺฉู ูโุชูุงูุฏ ุงุฒ OCR ฺฉ ุงุณฺฉุฑูโุดุงุชุ ุชุจุฏู ฺฉ ุตูุช ุจู ูุชู ุง ฺฉ ุงุฏุฏุงุดุช ุชุงูพโุดุฏู ุจุงุดุฏ) ู ุชุจุฏู ุขู ุจู ฺฉ ูุงู JSON ฺฉุงููุงู ุณุงุฎุชุงุฑุงูุชู ุจุฑ ุงุณุงุณ "ุงุณฺฉูุง ุฏุงูุด ุฌูุงู" (UKS) ุงุณุช.
        
        # ุจุฎุด ณ: ุชุนุฑู ุงุณฺฉูุง JSON - UKS (Defining the Output Format)
        ุดูุง ุจุงุฏ ุงุทูุงุนุงุช ุงุณุชุฎุฑุงุฌ ุดุฏู ุฑุง ุฏููุงู ุฏุฑ ูุงูุจ ุณุงุฎุชุงุฑ JSON ุฒุฑ ูุฑุงุฑ ุฏูุฏ:

        ```json
        {
          "core_content": {
            "title": "ฺฉ ุนููุงู ุจุณุงุฑ ฺฉูุชุงู ู ุชูุตู ุจุฑุง ุงู ุฏุงูุด (ุญุฏุงฺฉุซุฑ ฑฐ ฺฉููู).",
            "summary": "ฺฉ ุฎูุงุตู ฑ ุชุง ณ ุฌูููโุง ฺฉู ุฌุงู ฺฉูุงู ูุชู ูุฑูุฏ ุฑุง ุจุงู ูโฺฉูุฏ.",
            "original_text": "ูุชู ฺฉุงูู ู ุฎุงู ูุฑูุฏ ุจุฑุง ุขุฑุดู ู ุจุงุฒุจู."
          },
          "source_and_context": {
            "source_type": "ููุน ููุจุนุ ูุซูุง: Book, Podcast, Article, Video, Conversation, Personal Thought, Screenshot.",
            "source_name": "ูุงู ุฏูู ููุจุนุ ูุซูุง: 'Deep Work', 'Huberman Lab Podcast'. ุงฺฏุฑ ูุงูุดุฎุต ุจูุฏ null ูุฑุงุฑ ุจุฏู.",
            "source_author_or_creator": "ูุงู ููุณูุฏู ุง ุฎุงูู ุงุซุฑ. ุงฺฏุฑ ูุงูุดุฎุต ุจูุฏ null ูุฑุงุฑ ุจุฏู."
          },
          "categorization": {
            "primary_domain": "ุญูุฒู ุงุตู ูุฑุชุจุท ุจุง ุงู ุฏุงูุด. ููุท ฺฉ ุงุฒ ููุงุฑุฏ ูุณุช ูุฌุงุฒ ุงูุชุฎุงุจ ุดูุฏ.",
            "tags_and_keywords": ["ูุณุช ุงุฒ ุจุฑฺุณุจโูุง ู ฺฉููุงุช ฺฉูุฏ ุฏูู ฺฉู ุจู ุฌุณุชุฌู ุขูุฏู ฺฉูฺฉ ูโฺฉูุฏ."],
            "entities": ["ูุณุช ุงุฒ ููุฌูุฏุชโูุง ุฎุงุต ูุงู ุจุฑุฏู ุดุฏู ูุงููุฏ ุงุณุงู ุงูุฑุงุฏุ ูุญุตููุงุชุ ุดุฑฺฉุชโูุง ู..."]
          },
          "actionability": {
            "actionability_type": "ููุน ุงูุฏุงู ูุฑุชุจุท ุจุง ุงู ุฏุงูุด. ููุท ฺฉ ุงุฒ ููุงุฑุฏ ูุณุช ูุฌุงุฒ ุงูุชุฎุงุจ ุดูุฏ.",
            "action_item_description": "ุดุฑุญ ุฏูู ูุธูู ุฏุฑ ุตูุฑุช ฺฉู ูุงุจู ุงูุฏุงู ุจุงุดุฏ. ุฏุฑ ุบุฑ ุงู ุตูุฑุช null ูุฑุงุฑ ุจุฏู."
          }
        }
        # ุจุฎุด ด: ููุงูู ู ูุญุฏูุฏุชโูุง ฺฉูุฏ (Constraining the Model)
        ุจุฑุง ุงูุฌุงู ูุธูู ุฎูุฏุ ุดูุง ููุธู ุจู ุฑุนุงุช ููุงูู ุฒุฑ ูุณุชุฏ:

        ุฎุฑูุฌ ููุท JSON ุจุงุดุฏ: ูพุงุณุฎ ุดูุง ุจุงุฏ ููุท ู ููุท ฺฉ ุขุจุฌฺฉุช JSON ูุนุชุจุฑ ุจุงุดุฏ. ูฺ ูุชูุ ุชูุถุญ ุง ููุฏููโุง ูุจู ุง ุจุนุฏ ุงุฒ ุขุจุฌฺฉุช JSON ูููุณุฏ.

        ุฑุนุงุช ูุณุชโูุง ูุฌุงุฒ (Enums):

        ุจุฑุง ููุฏ primary_domainุ ููุท ฺฉ ุงุฒ ุงู ููุงุฏุฑ ุฑุง ุงุณุชูุงุฏู ฺฉู: ["YouTube", "Kaizen (Learning)", "Health & Lifestyle", "Finance (Crypto/Buying)", "Project Management", "Personal Journal (Ikigai)", "Other"]

        ุจุฑุง ููุฏ actionability_typeุ ููุท ฺฉ ุงุฒ ุงู ููุงุฏุฑ ุฑุง ุงุณุชูุงุฏู ฺฉู: ["Actionable Task", "Topic for Research", "Idea for Creation", "Information to Store", "Financial Record", "Personal Reflection"]

        ุนุฏู ุงุฎุชุฑุงุน ุงุทูุงุนุงุช: ูุฑฺฏุฒ ุงุทูุงุนุงุช ฺฉู ุฏุฑ ูุชู ูุฑูุฏ ูุฌูุฏ ูุฏุงุฑุฏ ุฑุง ุญุฏุณ ูุฒู ู ุจู ุฎุฑูุฌ ุงุถุงูู ูฺฉู. ุงฺฏุฑ ุงุทูุงุนุงุช ุจุฑุง ฺฉ ููุฏ ููุฌูุฏ ูุณุชุ ููุฏุงุฑ ุขู ุฑุง null ูุฑุงุฑ ุจุฏู.

        ุฒุจุงู ุฎุฑูุฌ: ุชูุงู ููุงุฏุฑ ูุชู ุฏุฑ ูุงู JSON ุจุงุฏ ุจู ุฒุจุงู ูุงุฑุณ ุจุงุดูุฏุ ูฺฏุฑ ุงูฺฉู ูุงู ฺฉ ููุฌูุฏุช ุฎุงุต (ูุงููุฏ "Deep Work") ุจู ุฒุจุงู ุงุตู ุจุงุดุฏ.
        # ุจุฎุด ต: ูุซุงูโูุง ุขููุฒุด (Few-Shot Learning)
        ูุซุงู ฑ: ูุฑูุฏ ุณุงุฏู ุงุฒ ฺฉ ฺฉุชุงุจ
        INPUT:
        "ุงุฏู ุจุงุดู ุงุฒ ฺฉุชุงุจ ุฏูพ ูุฑฺฉ ฺฉูู ูููพูุฑุช ุงู ูฺฉุชู ุฑู ุจุฑุง ูุฏู ูุฏุฑุช ุฒูุงูู ุงุณุชูุงุฏู ฺฉูู ฺฉู ูฺฏู ฺฉุงุฑ ุนูู ูุซู ฺฉ ุงุจุฑูุฏุฑุชู. ุจุงุฏ ู ููุช ูู ุจุฐุงุฑู ุฏุฑ ููุฑุฏุด ุจุดุชุฑ ุชุญูู ฺฉูู."
        JSON OUTPUT:
        {
          "core_content": {
            "title": "ฺฉุงุฑ ุนูู ุจู ุนููุงู ฺฉ ุงุจุฑูุฏุฑุช",
            "summary": "ูฺฉุชูโุง ุงุฒ ฺฉุชุงุจ 'ฺฉุงุฑ ุนูู' ุงุซุฑ ฺฉู ูููพูุฑุช ฺฉู ุจุงู ูโฺฉูุฏ ุชูุงูุง ุงูุฌุงู ฺฉุงุฑ ุนูู ฺฉ ูุฒุช ุฑูุงุจุช ู ุดุจู ุจู ฺฉ ุงุจุฑูุฏุฑุช ุฏุฑ ุฏูุง ุงูุฑูุฒ ุงุณุช. ุงู ูฺฉุชู ุจุฑุง ุงุณุชูุงุฏู ุฏุฑ ูุฏู ูุฏุฑุช ุฒูุงู ููุงุณุจ ุงุณุช.",
            "original_text": "ุงุฏู ุจุงุดู ุงุฒ ฺฉุชุงุจ ุฏูพ ูุฑฺฉ ฺฉูู ูููพูุฑุช ุงู ูฺฉุชู ุฑู ุจุฑุง ูุฏู ูุฏุฑุช ุฒูุงูู ุงุณุชูุงุฏู ฺฉูู ฺฉู ูฺฏู ฺฉุงุฑ ุนูู ูุซู ฺฉ ุงุจุฑูุฏุฑุชู. ุจุงุฏ ู ููุช ูู ุจุฐุงุฑู ุฏุฑ ููุฑุฏุด ุจุดุชุฑ ุชุญูู ฺฉูู."
          },
          "source_and_context": {
            "source_type": "Book",
            "source_name": "Deep Work",
            "source_author_or_creator": "Cal Newport"
          },
          "categorization": {
            "primary_domain": "YouTube",
            "tags_and_keywords": ["deep work", "productivity", "time management", "focus"],
            "entities": ["Cal Newport", "Deep Work"]
          },
          "actionability": {
            "actionability_type": "Topic for Research",
            "action_item_description": "ุชุญูู ุจุดุชุฑ ุฏุฑ ููุฑุฏ ููููู ฺฉุงุฑ ุนูู (Deep Work)."
          }
        }
        # ูุซุงู ฒ: ูุฑูุฏ ุชุฑฺฉุจ ู ูพฺุฏู
        INPUT:
        "ูุฏู ุฌุฏุฏ ููุฑ Aputure Amaran 150c. ููุช: ตถน ุฏูุงุฑ. CRI: 95+. ูุงุจูุช ฺฉูุชุฑู ุจุง ุงูพูฺฉุดู. ุงู ููุฑ ุนุงูู ุจุฑุง ฺฉุงูุงู ูุชูุจูุ ฺูู ุฑูฺฏ ูพูุณุช ุฑู ุฎู ุทุจุน ูุดูู ูุฏู. ุชู ุงูู ุจุฑุฑุณ ูฺฏูุช ุจุฑุง ุณูุงูุช ฺุดู ูู ุจูุชุฑ ุงุฒ ููุฑูุง ุงุฑุฒููโููุชูุ ฺูู ููฺฉุฑ ูุฏุงุฑู. ุจุงุฏ ุจุฐุงุฑูุด ุชู Buying List ู ุจุง ูุฏู Elgato Key Light ููุงุณู ฺฉูู. ุงู ู ุณุฑูุงูโฺฏุฐุงุฑ ุจุฑุง ฺฉุงุฑูู."
        JSON OUTPUT:
        {
          "core_content": {
            "title": "ุจุฑุฑุณ ููุฑ Aputure 150c ุจุฑุง ูุชูุจ ู ูุณุช ุฎุฑุฏ",
            "summary": "ููุฑ ุฌุฏุฏ Aputure Amaran 150c ุจุง ููุช ตถน ุฏูุงุฑุ ฺฉ ฺฏุฒูู ุนุงู ุจุฑุง ุงุณุชูุฏู ูุชูุจ ุงุณุช. ุงู ูุฏู ุจู ุฏูู ุดุงุฎุต CRI ุจุงูุง นตุ ุฑูฺฏ ูพูุณุช ุฑุง ุทุจุน ููุงุด ูโุฏูุฏ ู ุจู ุฎุงุทุฑ ุชฺฉููููฺ ุถุฏ ูุฑุฒุด (Flicker-Free)ุ ุจุฑุง ุณูุงูุช ฺุดู ูุฒ ููุฏ ุงุณุช. ุงู ููุฑุฏ ุจุงุฏ ุจู ุนููุงู ฺฉ ุณุฑูุงูโฺฏุฐุงุฑ ุฏุฑ ูุณุช ุฎุฑุฏ ุซุจุช ู ุจุง ุฑูุจุดุ Elgato Key Lightุ ููุงุณู ุดูุฏ.",
            "original_text": "ูุฏู ุฌุฏุฏ ููุฑ Aputure Amaran 150c. ููุช: ตถน ุฏูุงุฑ. CRI: 95+. ูุงุจูุช ฺฉูุชุฑู ุจุง ุงูพูฺฉุดู. ุงู ููุฑ ุนุงูู ุจุฑุง ฺฉุงูุงู ูุชูุจูุ ฺูู ุฑูฺฏ ูพูุณุช ุฑู ุฎู ุทุจุน ูุดูู ูุฏู. ุชู ุงูู ุจุฑุฑุณ ูฺฏูุช ุจุฑุง ุณูุงูุช ฺุดู ูู ุจูุชุฑ ุงุฒ ููุฑูุง ุงุฑุฒููโููุชูุ ฺูู ููฺฉุฑ ูุฏุงุฑู. ุจุงุฏ ุจุฐุงุฑูุด ุชู Buying List ู ุจุง ูุฏู Elgato Key Light ููุงุณู ฺฉูู. ุงู ู ุณุฑูุงูโฺฏุฐุงุฑ ุจุฑุง ฺฉุงุฑูู."
          },
          "source_and_context": {
            "source_type": "Video",
            "source_name": "ฺฉ ููุฏ ู ุจุฑุฑุณ ุขููุงู ุชุฌูุฒุงุช",
            "source_author_or_creator": null
          },
          "categorization": {
            "primary_domain": "YouTube",
            "tags_and_keywords": ["ุชุฌูุฒุงุช ูููโุจุฑุฏุงุฑ", "ููุฑูพุฑุฏุงุฒ", "ุณูุงูุช ฺุดู", "ูุณุช ุฎุฑุฏ", "ุณุฑูุงูโฺฏุฐุงุฑ"],
            "entities": ["Aputure Amaran 150c", "Elgato Key Light"]
          },
          "actionability": {
            "actionability_type": "Actionable Task",
            "action_item_description": "ูุฏู ููุฑ Aputure 150c ุฑุง ุฏุฑ 'Buying List' ุซุจุช ฺฉุฑุฏู ู ุขู ุฑุง ุจุง ูุฏู 'Elgato Key Light' ุงุฒ ูุธุฑ ููุช ู ูฺฺฏโูุง ููุงุณู ฺฉู."
          }
        }
        # ูุซุงู ณ: ูุฑูุฏ ุดุฎุต ู ฺฉูุชุงู (ููุฑุฏ ูุฑุฒ)
        INPUT:
        "ุงูุฑูุฒ ุฎู ุงุญุณุงุณ ุฎุณุชฺฏ ู ุจโุงูฺฏุฒฺฏ ูโฺฉูู. ููโุฏููู ฺุฑุง."
        JSON OUTPUT:
        {
          "core_content": {
            "title": "ุงุญุณุงุณ ุฎุณุชฺฏ ู ุจโุงูฺฏุฒฺฏ ุงูุฑูุฒ",
            "summary": "ุงุฏุฏุงุดุช ุดุฎุต ุฏุฑ ููุฑุฏ ุงุญุณุงุณ ุฎุณุชฺฏ ู ุจโุงูฺฏุฒฺฏ ุฏุฑ ุท ุฑูุฒ ุจุฏูู ุฏุงูุณุชู ุนูุช ูุดุฎุต ุขู.",
            "original_text": "ุงูุฑูุฒ ุฎู ุงุญุณุงุณ ุฎุณุชฺฏ ู ุจโุงูฺฏุฒฺฏ ูโฺฉูู. ููโุฏููู ฺุฑุง."
          },
          "source_and_context": {
            "source_type": "Personal Thought",
            "source_name": null,
            "source_author_or_creator": null
          },
          "categorization": {
            "primary_domain": "Personal Journal (Ikigai)",
            "tags_and_keywords": ["ุงุญุณุงุณุงุช", "ุฎุณุชฺฏ", "ุจโุงูฺฏุฒฺฏ", "ุฌูุฑูุงููฺฏ"],
            "entities": []
          },
          "actionability": {
            "actionability_type": "Personal Reflection",
            "action_item_description": null
          }
        }
        # ุจุฎุด ถ: ุฏุณุชูุฑ ููุง ู ูุฑูุฏ ฺฉุงุฑุจุฑ
        ุงฺฉูููุ ูุชู ุฎุงู ุฒุฑ ุฑุง ูพุฑุฏุงุฒุด ฺฉุฑุฏู ู ุฎุฑูุฌ JSON ูุฑุจูุทู ุฑุง ุชููุฏ ฺฉู:

        [<<ูุชู ุฎุงู ูุฑูุฏ ุงุฒ ฺฉุงุฑุจุฑ ุงูุฌุง ูุฑุงุฑ ูโฺฏุฑุฏ>>]
        """
        try:
            response = self.gemini_model.generate_content(prompt)
            json_text = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(json_text)
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุชุญูู ุจุง Gemini: {e}", exc_info=True)
            return None

    async def _create_calendar_event(self, entities: Dict[str, Any]) -> str:
        if not self.calendar_service:
            return "ุณุฑูุณ ุชููู ุฏุฑ ุฏุณุชุฑุณ ูุณุช."
        try:
            summary = entities.get("summary", "ุฑูุฏุงุฏ ุจุฏูู ุนููุงู")
            start_time_str = entities.get("start_time")
            if not start_time_str:
                return "ุฒูุงู ุฑูุฏุงุฏ ูุดุฎุต ูุดุฏ."
                
            start_time = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
            end_time = start_time + timedelta(hours=1)

            event = {
                'summary': summary,
                'start': {'dateTime': start_time.isoformat(), 'timeZone': 'UTC'},
                'end': {'dateTime': end_time.isoformat(), 'timeZone': 'UTC'},
            }
            created_event = self.calendar_service.events().insert(calendarId='primary', body=event).execute()
            return f"โ ุฑูุฏุงุฏ ยซ{summary}ยป ุจุง ููููุช ุฏุฑ ุชููู ฺฏูฺฏู ุซุจุช ุดุฏ."
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุงุฌุงุฏ ุฑูุฏุงุฏ ุชููู: {e}", exc_info=True)
            return "ูุดฺฉู ุฏุฑ ุงุฌุงุฏ ุฑูุฏุงุฏ ุชููู ูพุด ุขูุฏ."

    async def _add_to_knowledge_base(self, content: str) -> str:
        db_id = self.secrets.get('notion_kb_db_id')
        if not db_id: return "ุฎุทุง: ุดูุงุณู ุฏุชุงุจุณ ฺฉุชุงุจุฎุงูู ุฏุงูุด ุชุนุฑู ูุดุฏู ุงุณุช."

        try:
            logging.info(f"๐ง ุฏุฑ ุญุงู ุณุงุฎุช Embedding ุจุง ูุฏู {self.embedding_model}...")
            embedding_response = genai.embed_content(
                model=self.embedding_model,
                content=content,
                task_type="RETRIEVAL_DOCUMENT"
            )
            embedding_vector = embedding_response['embedding']
            embedding_str = json.dumps(embedding_vector)
            chunk_size = 2000
            embedding_chunks = [{"text": {"content": chunk}} for chunk in [embedding_str[i:i + chunk_size] for i in range(0, len(embedding_str), chunk_size)]]

            properties = {
                "Name": {"title": [{"text": {"content": content[:150]}}]},
                "Content": {"rich_text": [{"text": {"content": content}}]},
                "Embedding": {"rich_text": embedding_chunks}
            }
            self.notion.pages.create(parent={"database_id": db_id}, properties=properties)
            return "โ ูุทูุจ ุจุง ููููุช ุฏุฑ ฺฉุชุงุจุฎุงูู ุฏุงูุด ููุดู ุฐุฎุฑู ุดุฏ."
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุฐุฎุฑู ุฏุฑ ฺฉุชุงุจุฎุงูู ุฏุงูุด: {e}", exc_info=True)
            return "ูุดฺฉู ุฏุฑ ุฐุฎุฑู ุงุทูุงุนุงุช ุฏุฑ ููุดู ูพุด ุขูุฏ."

    async def _query_knowledge_base(self, query: str) -> str:
        db_id = self.secrets.get('notion_kb_db_id')
        if not db_id: return "ุฎุทุง: ุดูุงุณู ุฏุชุงุจุณ ฺฉุชุงุจุฎุงูู ุฏุงูุด ุชุนุฑู ูุดุฏู ุงุณุช."
        
        try:
            logging.info(f"๐ ุฏุฑ ุญุงู ุณุงุฎุช Embedding ุจุฑุง ูพุฑุณโูุฌู ุจุง {self.embedding_model}...")
            query_embedding_response = genai.embed_content(model=self.embedding_model, content=query, task_type="RETRIEVAL_QUERY")
            query_vector = np.array(query_embedding_response['embedding'])

            all_pages = []
            cursor = None
            while True:
                response = self.notion.databases.query(database_id=db_id, start_cursor=cursor)
                all_pages.extend(response.get('results', []))
                if not response.get('has_more'): break
                cursor = response.get('next_cursor')
            
            if not all_pages: return "ูพุงฺฏุงู ุฏุงูุด ุดูุง ุฎุงู ุงุณุช."
            logging.info(f"โ {len(all_pages)} ุตูุญู ุงุฒ ฺฉุชุงุจุฎุงูู ุฏุงูุด ุจุงุฒุงุจ ุดุฏ.")

            page_similarities = []
            for page in all_pages:
                props = page.get('properties', {})
                embedding_chunks = props.get("Embedding", {}).get('rich_text', [])
                if not embedding_chunks: continue
                embedding_json = "".join([chunk.get('text', {}).get('content', '') for chunk in embedding_chunks])
                try:
                    doc_vector = np.array(json.loads(embedding_json))
                    similarity = np.dot(query_vector, doc_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(doc_vector))
                    page_content = props.get("Content", {}).get('rich_text', [{}])[0].get('text', {}).get('content', 'ูุญุชูุง ููุฌูุฏ ูุณุช')
                    page_similarities.append((similarity, page_content))
                except (json.JSONDecodeError, ValueError) as e:
                    logging.warning(f"โ๏ธ ุฎุทุง ุฏุฑ ูพุฑุฏุงุฒุด Embedding ุจุฑุง ุตูุญู {page.get('id')}: {e}")
                    continue
            
            page_similarities.sort(key=lambda x: x[0], reverse=True)
            relevant_docs = [doc[1] for doc in page_similarities if doc[0] > 0.7][:3]
            
            if not relevant_docs: return "ูุชุงุณูุงูู ูุทูุจ ูุฑุชุจุท ุฏุฑ ูพุงฺฏุงู ุฏุงูุด ุดูุง ูพุฏุง ูฺฉุฑุฏู."
            logging.info(f"โ {len(relevant_docs)} ูฺฉุชู ูุฑุชุจุท ุจุฑุง ูพุงุณุฎโฺฏู ุงูุช ุดุฏ.")

            context_str = "\n\n---\n\n".join(relevant_docs)
            final_prompt = f"ุดูุง ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชุฏ ฺฉู ุจุฑ ุงุณุงุณ ูพุงฺฏุงู ุฏุงูุด ุดุฎุต ฺฉุงุฑุจุฑ ุจู ุณูุงูุงุช ูพุงุณุฎ ูโุฏูุฏ. ุจุฑ ุงุณุงุณ ยซูุชูโูุง ูุฑุชุจุทยป ุฒุฑุ ุจู ยซุณูุงู ฺฉุงุฑุจุฑยป ฺฉ ูพุงุณุฎ ุฌุงูุน ู ุฏูู ุจู ุฒุจุงู ูุงุฑุณ ุจุฏูุฏ.\n\n**ูุชูโูุง ูุฑุชุจุท ุงุฒ ูพุงฺฏุงู ุฏุงูุด:**\n{context_str}\n\n**ุณูุงู ฺฉุงุฑุจุฑ:**\n{query}\n\n**ูพุงุณุฎ ุดูุง (ุจู ูุงุฑุณ):**"
            
            logging.info("โ๏ธ ุฏุฑ ุญุงู ุชููุฏ ูพุงุณุฎ ููุง ุจุง Gemini...")
            final_response = self.gemini_model.generate_content(final_prompt)
            return final_response.text
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ูุฑุขูุฏ ูพุฑุณโูุฌู: {e}", exc_info=True)
            return f"ฺฉ ุฎุทุง ุบุฑููุชุธุฑู ุฏุฑ ููฺฏุงู ุฌุณุชุฌู ุฑุฎ ุฏุงุฏ: {e}"

    async def _process_user_request(self, text: str, update: Update):
        await update.message.reply_chat_action('typing')
        analysis = await self._analyze_text_with_gemini(text)
        
        if not analysis:
            await update.message.reply_text("โ ูุดฺฉู ุฏุฑ ุงุฑุชุจุงุท ุจุง ุณุฑูุณ ููุด ูุตููุน (Gemini) ูพุด ุขูุฏ.")
            return

        intent = analysis.get("intent")
        entities = analysis.get("entities", {})

        if intent == "CALENDAR_EVENT":
            response_text = await self._create_calendar_event(entities)
            await update.message.reply_text(response_text)
        elif intent == "KNOWLEDGE_STORAGE":
            content = entities.get("content", text)
            response_text = await self._add_to_knowledge_base(content)
            await update.message.reply_text(response_text)
        elif intent == "QUERY":
            query = entities.get("query", text)
            await update.message.reply_text("๐ ุฏุฑ ุญุงู ุฌุณุชุฌู ุฏุฑ ูพุงฺฏุงู ุฏุงูุด ุดูุง...")
            answer = await self._query_knowledge_base(query)
            await update.message.reply_text(answer)
        else:
            await update.message.reply_text("๐ค ูุชูุฌู ููุธูุฑ ุดูุง ูุดุฏู. ูุทูุงู ูุงุถุญโุชุฑ ุจุงู ฺฉูุฏ.")

    async def _convert_voice_to_text(self, voice_file_path: str) -> str:
        logging.info("๐ต ุฏุฑ ุญุงู ุชุจุฏู ุตุฏุง ุจู ูุชู...")
        try:
            audio = AudioSegment.from_ogg(voice_file_path)
            wav_path = voice_file_path + ".wav"
            audio.export(wav_path, format="wav")
            with sr.AudioFile(wav_path) as source:
                audio_data = self.recognizer.record(source)
            text = self.recognizer.recognize_google(audio_data, language='fa-IR')
            os.remove(wav_path)
            return text
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ุชุจุฏู ุตุฏุง ุจู ูุชู: {e}", exc_info=True)
            if 'wav_path' in locals() and os.path.exists(wav_path):
                os.remove(wav_path)
            return ""

    async def handle_voice_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        await update.message.reply_text("๐ค ูพุงู ุตูุช ุฏุฑุงูุช ุดุฏ. ูุทูุงู ุตุจุฑ ฺฉูุฏ...")
        voice = update.message.voice
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as temp_file:
            voice_file = await context.bot.get_file(voice.file_id)
            await voice_file.download_to_drive(temp_file.name)
            voice_path = temp_file.name
        
        text = await self._convert_voice_to_text(voice_path)
        os.unlink(voice_path)

        if text:
            await update.message.reply_text(f"๐ ูุชู ุดูุงุณุง ุดุฏู: ยซ{text}ยป")
            await self._process_user_request(text, update)
        else:
            await update.message.reply_text("โ ูุชุงุณูุงูู ูุชูุงูุณุชู ุตุฏุงุชุงู ุฑุง ุชุดุฎุต ุฏูู.")

    async def handle_text_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        user_text = update.message.text
        logging.info(f"โจ๏ธ ูพุงู ูุชู ุฏุฑุงูุช ุดุฏ: '{user_text}'")
        await self._process_user_request(user_text, update)

    async def handle_photo_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        await update.message.reply_text("๐ผ๏ธ ุชุตูุฑ ุฏุฑุงูุช ุดุฏ. ุฏุฑ ุญุงู ุงุณุชุฎุฑุงุฌ ูุชู ุจุง Gemini...")
        photo_file = await context.bot.get_file(update.message.photo[-1].file_id)
        with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as temp_photo:
            await photo_file.download_to_drive(temp_photo.name)
            photo_path = temp_photo.name
        
        try:
            img = Image.open(photo_path)
            response = self.gemini_model.generate_content(["Extract all text from this image in Persian.", img])
            extracted_text = response.text
            os.unlink(photo_path)
            
            if extracted_text and extracted_text.strip():
                await update.message.reply_text(f"๐ ูุชู ุงุณุชุฎุฑุงุฌ ุดุฏู:\n\nยซ{extracted_text}ยป\n\n๐ ุฏุฑ ุญุงู ุงูุฒูุฏู ุจู ฺฉุชุงุจุฎุงูู ุฏุงูุด...")
                response_text = await self._add_to_knowledge_base(extracted_text)
                await update.message.reply_text(response_text)
            else:
                await update.message.reply_text("ูุชู ุฏุฑ ุชุตูุฑ ุงูุช ูุดุฏ.")
        except Exception as e:
            logging.error(f"โ ุฎุทุง ุฏุฑ ูพุฑุฏุงุฒุด ุชุตูุฑ: {e}", exc_info=True)
            await update.message.reply_text("ูุดฺฉู ุฏุฑ ูพุฑุฏุงุฒุด ุชุตูุฑ ูพุด ุขูุฏ.")
            if os.path.exists(photo_path):
                os.unlink(photo_path)

    async def run(self):
        logging.info("\n๐ ุฏุฑ ุญุงู ุฑุงูโุงูุฏุงุฒ ุฑุจุงุช ุชูฺฏุฑุงู...")
        app = Application.builder().token(self.secrets['telegram']).build()
        
        app.add_handler(MessageHandler(filters.VOICE, self.handle_voice_message))
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_text_message))
        app.add_handler(MessageHandler(filters.PHOTO, self.handle_photo_message))
        
        logging.info("๐ฅ ุฑุจุงุช ุจุง ููููุช ูุนุงู ุดุฏ! ุขูุงุฏู ุฏุฑุงูุช ูพุงู.")
        await app.run_polling()
